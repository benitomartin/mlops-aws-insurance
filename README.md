# MLOPS MEDICAL INSURANCE COSTS PREDICTION ‚ö±Ô∏è

<p align="center">
  <img width="976" alt="aws" src="https://github.com/benitomartin/benitomartin/assets/116911431/a620b68c-30a2-4666-986e-642046ea435b">
</p>

This is a personal MLOps project based on a [Kaggle](https://www.kaggle.com/datasets/mirichoi0218/insurance) dataset for medical insurance costs prediction. It contains several AWS SageMaker pipelines from preprocessing till deployment, inference and monitoring.

Feel free to ‚≠ê and clone this repo üòâ

## Tech Stack

![Visual Studio Code](https://img.shields.io/badge/Visual%20Studio%20Code-0078d7.svg?style=for-the-badge&logo=visual-studio-code&logoColor=white)
![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-%23d9ead3.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)
![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=for-the-badge&logo=scikit-learn&logoColor=white)
![Flask](https://img.shields.io/badge/flask-%23000.svg?style=for-the-badge&logo=flask&logoColor=white)
![Anaconda](https://img.shields.io/badge/Anaconda-%2344A833.svg?style=for-the-badge&logo=anaconda&logoColor=white)
![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=white)
![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)
![Git](https://img.shields.io/badge/git-%23F05033.svg?style=for-the-badge&logo=git&logoColor=white)

## Project Structure

The project has been structured with the following folders and files:

- `.github/workflows`: contains the CI/CD files (GitHub Actions)
- `aws_pipelines`: AWS pipelines from preprocessing till deployment and monitoring
  - ‚úÖ `preprocessing_pipeline.py`: data preprocessing
  - ‚úÖ `training_pipeline.py`: model training
  - ‚úÖ `tuning_pipeline.py`: model fine tuning
  - ‚úÖ `evaluate_pipeline.py`: model evaluation
  - ‚úÖ `register_pipeline.py`: model registry
  - ‚úÖ `cond_register_pipeline.py`: model conditional registry (based on MAE Threshold)
  - ‚úÖ `deployment_pipeline.py`: model automatic deployment
  - ‚úÖ `manual_deployment_pipeline.py`:  model manual deployment (requires manual approval on AWS)
  - ‚úÖ `inference_pipeline.py`: model automatic deployment and endpoint creation
  - ‚úÖ `data_quality_pipeline.py`:  model registry with data quality baseline
  - ‚úÖ `model_quality_pipeline.py`:  model registry with data and model quality baseline
  - ‚úÖ `monitoring_pipeline.py`: data and model monitor schedules creation
- `data:` raw and clean data
- `Notebooks`: Exploratory Data Analysis
- `src:` code_scripts for processing, training, evaluation, serving (Flask), lambda, inference and endpoint testing
- `.env_sample`: sample environmental variables
- `.flake8`: flake requirements
- `.gitattributes`: gitattributes
- `Makefile`: install requirements, formating, testing, linting, coverage report and clean up
- `pyproject.toml`: linting and formatting
- `requirements.txt:` project requirements

## Project Description

The dataset was obtained from Kaggle and contains 1338 rows and 7 columns to predict health insurance costs. To prepare the data for modelling, an **Exploratory Data Analysis** was conducted. For modeling, the categorical features where encoded, Tensorflow was use as model and the mean absolute error threshold was selected for model registry.

<p align="center">
    <img src="https://github.com/benitomartin/mlops-car-prices/assets/116911431/068fef53-763c-45ea-a94b-6469fca2269f">
</p>

## Project Set Up

The Python version used for this project is Python 3.10.

1. Clone the repo (or download it as a zip file):

   ```bash
   git clone https://github.com/benitomartin/mlops-aws-insurance.git
   ```

2. Create the virtual environment named `main-env` using Conda with Python version 3.10:

   ```bash
   conda create -n main-env python=3.10
   conda activate main-env
   ```

3. Execute the `Makefile` script and install the project dependencies included in the requirements.txt:

    ```bash
    pip install -r requirements.txt

    or
 
    make install
    ```

Additionally, please note that an **AWS Account**, credentials, and proper policies with full access to SageMaker, S3, and Lambda are necessary for the projects to function correctly. Make sure to configure the appropriate credentials to interact with AWS services.

## Pipeline Deployment

All pipelines where deployed on AWS SageMaker, as well as the Model Registry and Endpoints. At the end of each pipeline the is a line that must be uncommented to run it on AWS:

    # Start the pipeline execution (if required)
    evaluation_pipeline.start()

Additionally the experiments were tracked on Comel ML.

<p align="center">
    <img src="https://github.com/benitomartin/mlops-car-prices/assets/116911431/4329f6c9-4372-44e3-8b46-0f16e88f00cd">
    </p>
